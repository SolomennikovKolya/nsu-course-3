(Теория и практика нейронных сетей)

### Полезности
- [Google meet](https://meet.google.com/bqn-vtqa-qyp)
- [Табличка по лекциям](https://docs.google.com/spreadsheets/d/1WCa9pSmM-XS3a9sGbDXDgUYXFKgSINKzeVKHvyuMFYI/edit?gid=0#gid=0)
- [Табличка по семинарам 1](https://docs.google.com/spreadsheets/d/14yCJYLOlQd8IGoXpF4U1BA3V9K750hh6dkvqLFyFMkg/edit?gid=0#gid=0)
- [Табличка по семинарам 2](https://docs.google.com/spreadsheets/d/1rN-egnV9wQpLekzAwKIx0SPXYv0-B6k5DxUkDEE0u0Q/edit?gid=0#gid=0)
- [Программа курса](https://docs.google.com/document/d/1ZPGZz4EKvNUlIRXbwcw65BgoO1bEIpYR/edit)
- [Описание практических работ](https://docs.google.com/document/d/1A-21IyDohCQkSAykN3H8avDLWJPRdXO7/edit#heading=h.gjdgxs)
- [Доп материалы](https://docs.google.com/document/d/1FBNpuNCz2kkMdNkGERenytFApZwmqoBqT1lq5HGV1B0/edit?usp=sharing)
- [Датасеты](https://drive.google.com/drive/folders/1jZ_ujOF1XfcUwP4k7jMlKu1w6joPHMsy)
- [Теория по подготовке датасетов](https://docs.google.com/presentation/d/10QrGbD6f9Qwi4u3AWNyB61Udj-7AZA0x/edit#slide=id.p1)

#### Инфа
- Мой датасет: Качество вина
- Моя тема доклада: Архитектура RNN
- Kaggle - там можно брать датасеты
- Colab - питончик + ML
- s.uzilov@g.nsu.ru - почта семера, куда скидывать презентации

## Лекции

### Термины
- **Зависимая переменная** - основной фактор в машинном обучении, который мы хотим предсказать или понять, называется зависимой переменной. Ее также называют целевой переменной
- **Независимая переменная** - факторы, которые влияют на зависимые переменные или которые используются для прогнозирования значений зависимых переменных, называются независимыми переменным. также называемыми предикторами
- **Выброс** - это наблюдение, которое содержит либо очень низкое, либо очень высокое значение по сравнению с другими наблюдаемыми значениями. Выброс может исказить результат, поэтому его следует избегать
- **Мультиколлинеарность** - если независимые переменные сильно коррелируют друг с другом, чем с другими переменными, то такое состояние называется мультиколлинеарностью. Его не должно быть в наборе данных, потому что это создает проблемы при ранжировании наиболее влияющей переменной
- **Недообучение** и **переобучение** - если наш алгоритм хорошо работает с обучающим набором данных, но плохо работает с тестовым набором данных, то такая проблема называется переобучением. И если наш алгоритм плохо работает даже с обучающим набором данных, то такая проблема называется недообучением

#### Независимая переменная
- *Определение*: Независимые переменные — это входные данные или признаки (features), которые используются для предсказания или объяснения зависимой переменной
- *Роль*: Они представляют собой входные параметры модели, на основе которых делаются прогнозы
- *Пример*: Если вы предсказываете цену дома (зависимая переменная), то независимыми переменными могут быть площадь дома, количество комнат, район расположения и т.д

#### Зависимая переменная
- *Определение*: Зависимая переменная — это целевая переменная (target), которую модель пытается предсказать или объяснить на основе независимых переменных
- *Роль*: Это выход модели, который зависит от входных данных (независимых переменных)
- *Пример*: В задаче предсказания цены дома зависимой переменной будет сама цена дома

### Датасет

#### Распределение датасета
![[Pasted image 20250304133304.png]]

#### Кросс-валидация
![[Pasted image 20250218132707.png]]

#### Validation set помогает
- Контролировать процесс обучения
- Настраивать гиперпараметры
- Избегать переобучения
- Сохранять объективность тестового набора для финальной оценки

#### Алгоритм C4.5

**Алгоритм C4.5** — это популярный алгоритм построения деревьев решений. Он является улучшенной версией алгоритма ID3 и широко используется в машинном обучении для задач классификации и регрессии. C4.5 поддерживает как категориальные, так и числовые признаки, а также умеет обрабатывать пропущенные данные

##### Основные особенности
1. *Использование критерия Gain Ratio*:
    - В отличие от ID3, который использует критерий Information Gain, C4.5 использует Gain Ratio, чтобы избежать перекоса в сторону признаков с большим количеством значений
2. *Поддержка числовых признаков*:
    - C4.5 автоматически находит оптимальные точки разделения для числовых признаков
3. *Обработка пропущенных данных*:
    - Алгоритм может работать с пропущенными значениями, используя взвешенное распределение
4. *Упрощение дерева (pruning)*:
    - C4.5 включает этап пост-обработки для упрощения дерева, что помогает избежать переобучения
5. *Поддержка весов*:
    - Алгоритм учитывает веса объектов, что полезно для работы с несбалансированными данными

##### Основные шаги алгоритма
1. *Выбор признака для разделения*:
    - Для каждого признака вычисляется Gain Ratio
    - Признак с максимальным Gain Ratio выбирается для разделения
2. *Разделение данных*:
    - Если признак категориальный, данные разделяются по его значениям
    - Если признак числовой, находится оптимальная точка разделения
3. *Рекурсивное построение дерева*:
    - Процесс повторяется для каждого подмножества данных, пока не будут выполнены условия остановки
4. *Упрощение дерева (pruning)*:
    - Удаляются лишние узлы, чтобы избежать переобучения

### Регрессия

Задача регрессии в машинном обучении (ML) — это тип задачи, в которой цель заключается в предсказании непрерывной числовой величины на основе входных данных. В отличие от задачи классификации, где предсказывается категория или класс, в регрессии результатом является число

#### Основные компоненты задачи регрессии
1. **Входные данные (признаки)**:
    - Это независимые переменные (features), которые используются для предсказания целевой переменной. Например, в задаче предсказания цены дома признаками могут быть площадь дома, количество комнат, район и т.д.
2. **Целевая переменная**:
    - Это зависимая переменная, которую мы хотим предсказать. В задаче регрессии она всегда является числовой. Например, цена дома, температура, доход и т.д.
3. **Модель регрессии**:
    - Это математическая функция, которая связывает входные данные с целевой переменной. Модель обучается на данных, чтобы минимизировать ошибку предсказания.
4. **Функция потерь (Loss function)**:
    - Это метрика, которая измеряет, насколько предсказания модели отличаются от реальных значений. Для регрессии часто используется среднеквадратичная ошибка (MSE — Mean Squared Error) или средняя абсолютная ошибка (MAE — Mean Absolute Error).
5. **Обучение модели**:
    - Процесс настройки параметров модели для минимизации функции потерь на обучающих данных.
        
#### Примеры задач регрессии:
1. *Предсказание цены дома*:
    - Признаки: площадь, количество комнат, этаж, район
    - Целевая переменная: цена дома
2. *Прогнозирование температуры*:
    - Признаки: время года, влажность, давление
    - Целевая переменная: температура
3. *Оценка времени доставки*:
    - Признаки: расстояние, тип доставки, загруженность дорог
    - Целевая переменная: время доставки

#### Популярные алгоритмы регрессии
1. **Линейная регрессия**:
    - Простейшая модель, которая предполагает линейную зависимость между признаками и целевой переменной
2. **Полиномиальная регрессия**:
    - Расширение линейной регрессии, где зависимость моделируется полиномом
3. **Метод опорных векторов (SVR — Support Vector Regression)**:
    - Используется для нелинейных данных
4. **Регрессия на основе деревьев**:
    - Например, Decision Tree Regression, Random Forest Regression, Gradient Boosting Regression (XGBoost, LightGBM, CatBoost)
5. **Нейронные сети**:
    - Глубокие нейронные сети могут использоваться для сложных задач регрессии

#### Оценка качества модели
##### Сравнение метрик
![[Pasted image 20250225140210.png]]

### Классификация

**Классификация** — это задача, которая заключается в отнесении объектов (например, данных) к одному из заранее определённых классов на основе их признаков. Это одна из основных задач в supervised learning (обучении с учителем), где модель обучается на размеченных данных (с известными метками классов), а затем используется для предсказания классов новых объектов

#### Основные подходы к классификации:
1. *Логистическая регрессия* — несмотря на название, используется для бинарной классификации
2. *Метод k-ближайших соседей (k-Nearest Neighbors, k-NN)* — ленивый алгоритм, который классифицирует объект на основе классов его ближайших соседей
3. *Деревья решений* — алгоритм, который строит иерархическую структуру решений на основе признаков
4. *Случайный лес (Random Forest)* — ансамбль деревьев решений, который улучшает точность и устойчивость модели
5. *Метод опорных векторов (Support Vector Machines, SVM)* — ищет гиперплоскость, которая наилучшим образом разделяет классы
6. *Нейронные сети* — мощный инструмент для сложных задач классификации, особенно в глубоком обучении

#### Ленивое обучение (Lazy Learning)

Ленивые алгоритмы не строят явную модель во время обучения. Вместо этого они откладывают всю обработку до момента, когда поступает новый объект для классификации. Примером такого подхода является метод k-ближайших соседей (k-NN)

- **Как работает k-NN**:
    - На этапе обучения алгоритм просто запоминает все данные
    - На этапе предсказания для нового объекта вычисляются расстояния до всех объектов в обучающей выборке
    - Выбираются k ближайших объектов, и класс нового объекта определяется на основе классов этих соседей (например, путём голосования)
- **Преимущества**:
    - Простота реализации
    - Нет необходимости строить сложную модель
    - Легко адаптируется к новым данным
- **Недостатки**:
    - Медленное предсказание, так как нужно вычислять расстояния до всех объектов
    - Чувствительность к шуму и выбросам
    - Требует много памяти для хранения всей обучающей выборки

#### Жадное обучение (Eager Learning)

Жадные алгоритмы строят явную модель на этапе обучения, которая затем используется для предсказания. Примеры: деревья решений, логистическая регрессия, SVM, нейронные сети

- **Как работает жадное обучение**:
    - На этапе обучения алгоритм анализирует данные и строит модель (например, дерево решений или гиперплоскость в SVM)
    - На этапе предсказания модель применяется к новым данным для определения их класса
- **Преимущества**:
    - Быстрое предсказание, так как модель уже построена
    - Меньше требований к памяти, так как хранится только модель, а не все данные
    - Лучше работает с большими объёмами данных
- **Недостатки**:
    - Модель может быть сложной для интерпретации (например, глубокие нейронные сети)
    - Требуется время и ресурсы для построения модели
    - Модель может быть чувствительна к изменениям в данных

#### Классификация на линейные и нелинейные модели

**Линейные модели** предполагают, что зависимость между признаками и целевой переменной можно описать линейной комбинацией признаков. Такие модели часто интерпретируемы и просты в реализации, но они могут быть ограничены в задачах, где зависимости сложные и нелинейные

Примеры:
1. Линейная регрессия
2. Логистическая регрессия
3. Метод опорных векторов (SVM) с линейным ядром

**Нелинейные модели** способны учитывать сложные зависимости между признаками и целевой переменной. Они более гибкие и мощные, но часто требуют больше вычислительных ресурсов и могут быть менее интерпретируемыми

Примеры
1. Деревья решений
2. Случайный лес (Random Forest)
3. Метод опорных векторов (SVM) с нелинейным ядром
4. Нейронные сети
5. Метод k-ближайших соседей (k-NN)

#### Оценка модели классификации

Как только наша модель будет завершена, необходимо оценить ее производительность; либо это классификационная, либо регрессионная модель.
Итак, для оценки модели классификации у нас есть следующие способы:

1. Логарифм потерь или кросс-энтропийная потеря (log loss)
2. Матрица путаницы
3. Кривая АИС-ВОС (кривая рабочих характеристик получателя)

#### Логистическая регрессия

**Логистическая регрессия** — это один из самых популярных алгоритмов машинного обучения, используемый для задач *бинарной классификации* (когда целевая переменная имеет два класса, например, "да/нет", "спам/не спам", "1/0"). Несмотря на название, это именно алгоритм классификации, а не регрессии

Основная идея:
Логистическая регрессия предсказывает вероятность принадлежности объекта к одному из двух классов. Для этого она использует логистическую функцию (сигмоиду), которая преобразует линейную комбинацию признаков в значение вероятности в диапазоне от [0, 1]

Математическая основа
1. *Линейная комбинация признаков*:  
    Логистическая регрессия сначала вычисляет линейную комбинацию входных признаков:
    z=w0+w1x1+w2x2+⋯+wnxn​, где:
    - zz — линейная комбинация
    - w0,w1,…,wn​ — веса (параметры модели)
    - x1,x2,…,xn​ — признаки объекта
2. *Логистическая функция (сигмоида)*:  
    Затем линейная комбинация z преобразуется с помощью логистической функции. Сигмоида "сжимает" значение zz в диапазон от 0 до 1, что интерпретируется как вероятность
3. *Принятие решения*:
    - Если p(y=1)≥0.5, объект классифицируется как класс 1
    - Если p(y=1)<0.5, объект классифицируется как класс 0

Обучение модели:
Логистическая регрессия обучается путём нахождения оптимальных весов w0,w1,…,wnw0​,w1​,…,wn​, которые минимизируют функцию потерь (обычно логистическую потерю или перекрёстную энтропию)

#### Оценка качества модели

Полнота (Recall) и точность (Precision) — это две важные метрики для оценки качества моделей классификации, особенно в задачах, где классы не сбалансированы (например, когда один класс встречается значительно реже другого). Эти метрики помогают понять, насколько хорошо модель справляется с предсказанием положительного класса

##### Основные понятия
1. **True Positive (TP)**:
    - Модель правильно предсказала положительный класс
    - Пример: модель предсказала, что пациент болен, и он действительно болен
2. **False Positive (FP)**:
    - Модель неправильно предсказала положительный класс
    - Пример: модель предсказала, что пациент болен, но он здоров
3. **True Negative (TN)**:
    - Модель правильно предсказала отрицательный класс
    - Пример: модель предсказала, что пациент здоров, и он действительно здоров
4. **False Negative (FN)**:
    - Модель неправильно предсказала отрицательный класс
    - Пример: модель предсказала, что пациент здоров, но он болен

**Точность (Precision)** показывает, какая доля объектов, предсказанных как положительные, действительно является положительными
- Формула: `Precision=TP/(TP+FP)`
- Отвечает на вопрос: "Насколько мы можем доверять предсказаниям модели?"
- Точность важна, когда ложные срабатывания (FP) дорого обходятся. Например, в спам-фильтрах: лучше пропустить спам (FN), чем пометить важное письмо как спам (FP)

**Полнота (Recall)** показывает, какая доля всех реальных положительных объектов была правильно предсказана моделью
- Формула: `Recall=TP/(TP+FN)`
- Отвечает на вопрос: "Сколько из всех положительных объектов мы нашли?"
- Полнота важна, когда пропуски (FN) критичны. Например, в медицине: лучше ошибиться и предсказать болезнь у здорового пациента (FP), чем пропустить болезнь (FN).

**F1-мера** — среднее гармоническое между точностью и полнотой
- Формула: `F1=2*Precission*Recall/(Precission+Recall)`
- Пригождается, так как точность и полнота часто находятся в компромиссе: увеличение одной метрики может привести к уменьшению другой
