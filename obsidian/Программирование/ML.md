
### Полезности
- [Введение в ML](https://habr.com/ru/articles/448892/)
- [OpenCV](https://www.youtube.com/watch?v=_23gclr91P4&list=PL0lO_mIqDDFUAQ2RdAgLp6Tj_fREcxk6T)

### Основные определения

#### Типы задач машинного обучения
- **Задача регрессии** – прогноз на основе выборки объектов с различными признаками
	- На выходе - вещественное число
	- Например - цена квартиры, стоимость акции, ожидаемый доход
- **Задача классификации** – получение категориального ответа на основе набора признаков
	- На выходе - конечное количество ответов (как правило, в формате «да» или «нет»)
	- Например - есть ли на фотографии кот, болен ли пациент
- **Задача кластеризации** – распределение данных на группы
	- На выходе - группировка объектов в кластеры, так чтобы объекты внутри одного кластера были более похожи друг на друга, чем на объекты из других кластеров.
	- Наприимер - разделение всех клиентов мобильного оператора по уровню платёжеспособности, отнесение космических объектов к той или иной категории (планета, звёзда, чёрная дыра и т. п.)
- **Задача уменьшения размерности** – сведение большого числа признаков к меньшему (обычно 2–3) для удобства их последующей визуализации
	- На выходе - данные с меньшей размерностью
	- Например - сжатие данных
- **Задача выявления аномалий** – отделение аномалий от стандартных случаев (методы классификации здесь не работают)
	- Например - выявление мошеннических действий с банковскими картами
- **Ранжирование** - упорядочивание данных

#### Виды машинного обучения
1. **С учителем** - то есть с вмешательством человека в процесс обработки информации
2. **Без учителя** - без вмешательста человека (на поставленную задачу не даётся готовых правильных ответов)

**Датасет** — это весь набор данных, который используется в процессе разработки модели машинного обучения. В него входит:
- **Обучающая выборка** — для обучения модели
- **Тестовая выборка** — для оценки качества модели на новых данных, которые модель не видела в процессе обучения
- **Валидационная выборка** — иногда выделяется для настройки гиперпараметров и проверки качества модели в процессе обучения, чтобы избежать переобучения.

## Основные методы машинного обучения

### Дерево принятия решений

**Дерево принятия решений** — это один из методов машинного обучения, который используется как для задач классификации, так и для регрессии. Это модель, основанная на иерархической структуре, которая последовательно принимает решения, разделяя данные по различным признакам.

Модель представляется в виде дерева, где:
1. **Корневой узел** — это первый узел дерева, с которого начинается процесс принятия решений. Он содержит весь датасет и первый признак для разделения данных.
2. **Внутренние узлы** — это точки в дереве, где данные делятся на подмножества на основе условий по признакам. Каждый узел проверяет какое-либо условие (например, "возраст > 30?") и направляет данные в один из двух (или более) дочерних узлов.
3. **Листовые узлы** — это конечные узлы дерева, которые представляют решение. В случае классификации листовые узлы содержат метки классов, а в случае регрессии — значения, которые предсказываются.

#### Этапы обучения дерева принятия решений:
1. Выбор признаков для разделения данных:
    - На каждом шаге обучения модель выбирает признак из набора доступных признаков, по которому данные будут лучше всего разделяться на подгруппы
    - Для этого используется метрика, которая оценивает "качество" разделения. В задачах классификации наиболее популярные метрики:
        - **Критерий Джини** (Gini impurity): измеряет вероятность неправильной классификации случайно выбранного элемента, если он был классифицирован по данному признаку
        - **Информационная энтропия** (Information Gain): измеряет уменьшение неопределенности или "хаоса" после разделения
	    - **Метрика среднеквадратической ошибки** (MSE) - часто используется в задачах регрессии для оценки качества разбиения

2. Поиск пороговых значений:
    - Для каждого числового признака выбирается пороговое значение, которое разделяет данные на две группы. Например, если признак — "возраст", модель может выбрать порог "возраст > 30" или "доход > 50,000"
    - Пороговое значение подбирается таким образом, чтобы минимизировать "нечистоту" групп, т.е. чтобы каждая группа была как можно более однородной с точки зрения целевой переменной

3. Разделение данных:
    - После выбора наилучшего признака и порога, данные разделяются на две (или больше) подгруппы, которые соответствуют условиям на текущем узле. Модель продолжает строить дерево, повторяя этот процесс для каждой подгруппы данных.

4. Остановка роста дерева:
    - Процесс продолжается до тех пор, пока не будет достигнуто одно из условий остановки:
        - Листовой узел содержит слишком мало данных для дальнейшего разделения.
        - Все данные в узле принадлежат к одному и тому же классу (в задачах классификации).
        - Достигнута максимальная глубина дерева.
        - Дальнейшее разделение не приносит значимого улучшения качества (по метрике разбиения).

5. Формирование конечного дерева:
    - Когда дерево построено, на листовых узлах хранятся предсказания (например, метки классов в задачах классификации или предсказанные значения в задачах регрессии).

#### Преимущества:
- Простота интерпретации: Деревья принятия решений легко визуализировать и объяснить
- Работа с различными типами данных: Могут работать как с числовыми, так и с категориальными признаками
- Не требует масштабирования данных: Не требует нормализации данных перед обучением

#### Недостатки:
- Склонность к переобучению: Если дерево слишком глубокое, оно может переобучиться, запоминая детали обучающей выборки и плохо обобщая данные
- Чувствительность к изменениям в данных: Небольшие изменения в данных могут привести к существенным изменениям структуры дерева

#### Области применения:
- Задачи классификации
- Задачи регрессии

### Наивная байесовская классификация  
  
**Наивная байесовская классификация** — это простой и эффективный алгоритм классификации, основанный на применении теоремы Байеса с предположением о независимости признаков. Это предположение называется "наивным", потому что на практике признаки часто не являются полностью независимыми, однако алгоритм всё равно показывает хорошие результаты в ряде задач, таких как текстовая классификация, фильтрация спама, анализ настроений и другие.

#### Теорема Байеса
Формулировка: `P(C∣X) = P(X∣C) * P(C) / P(X)​`
- `P(C∣X)` — **апостериорная вероятность** (вероятность того, что объект принадлежит классу C при условии наблюдения признаков X)
- `P(X∣C)` — **правдоподобие** (вероятность наблюдать признаки X, если известно, что объект принадлежит классу C)
- `P(C)` — **априорная вероятность** класса C (предполагаемая вероятность того, что объект принадлежит классу C без учёта признаков)
- `P(X)` — **нормирующий фактор**, который служит для того, чтобы апостериорные вероятности суммировались в единицу (его можно игнорировать в процессе классификации, так как он одинаков для всех классов)

#### Преимущества:
- Простота: Алгоритм легко реализуется и требует небольших вычислительных ресурсов.
- Быстрота: Наивный Байес очень быстро обучается и делает предсказания, что делает его особенно полезным для работы с большими датасетами.
- Эффективность на текстовых данных: Наивный Байес хорошо работает в задачах классификации текста, таких как анализ настроений, фильтрация спама и категоризация документов, благодаря своей способности обрабатывать множество признаков (слов) одновременно.
- Устойчивость к шуму: Модель может хорошо справляться с большим количеством незначимых признаков.

#### Недостатки:
- Наивное предположение о независимости признаков: В реальности признаки часто зависят друг от друга, и это упрощённое предположение может снижать точность модели. Например, в тексте слова часто взаимосвязаны (контекст), но наивный Байес не учитывает такие связи.
- Низкая точность на некоторых типах данных: В задачах, где признаки не независимы или есть сложные взаимосвязи между ними, наивный Байес может работать хуже, чем более сложные модели, такие как деревья решений, случайные леса или нейронные сети.

#### Области применения
- Задачи классификации, где признаки слабо зависят друг от друга
- Примеры:
	- Определение спама, приходящего на электронную почту;
	- Автоматическая привязка новостных статей к тематическим рубрикам;
	- Выявление эмоциональной окраски текста;
	- Распознавание лиц и других паттернов на изображениях.

### Метод наименьших квадратов  

**Метод наименьших квадратов** — это один из основных методов для оценки параметров моделей в задачах регрессии. Цель метода заключается в том, чтобы найти такие параметры модели, которые минимизируют сумму квадратов отклонений между реальными значениями и предсказанными моделью.

Этот метод широко используется в задачах линейной регрессии, где предполагается, что зависимость между целевой переменной y и признаками X можно описать с помощью линейной модели.

![[Pasted image 20241018203346.png]]

#### Линейная модель
**Линейная модель** — это модель, которая описывает зависимость между одной или несколькими независимыми переменными (признаками) и целевой переменной с помощью линейной функции. В линейной модели целевая переменная является линейной комбинацией входных признаков. Это один из самых простых и широко используемых типов моделей в статистике и машинном обучении.

Форма линейной модели для случая с одной независимой переменной (простая линейна регрессия):
`y = β0 + β1x + ϵ`
- `y` — целевая переменная (то, что мы пытаемся предсказать)
- `x` — независимая переменная (признак)
- `β0`​ — свободный член (интерсепт), который показывает значение y, когда x = 0
- `β1`​ — коэффициент наклона, который показывает, на сколько изменится y при изменении x на единицу
- `ϵ` — ошибка или шум модели, учитывающий возможные отклонения реальных данных от предсказанных

Если у нас несколько независимых переменных (признаков), модель называется множественной линейной регрессией и записывается следующим образом:
`y = β0 + β1*x1 + β2*x2 + ⋯ + βp*xp + ϵ`
- `x1,x2,…,xp`​ — независимые переменные (признаки)
- `β1,β2,…,βp`​ — коэффициенты модели, которые показывают, как каждый признак влияет на целевую переменную y

#### Преимущества:
- Простота: Метод легко реализуется и интерпретируется.
- Эффективность: Для линейных зависимостей между переменными этот метод даёт хорошие результаты.
- Аналитическое решение: Для линейной регрессии существует точное решение (формулы), что делает метод быстрым и эффективным с вычислительной точки зрения.

#### Недостатки:
- Чувствительность к выбросам: Из-за того, что отклонения возводятся в квадрат, метод наименьших квадратов очень чувствителен к аномальным данным (выбросам), так как большие ошибки увеличивают значение функции ошибки.
- Линейность: Метод предполагает линейную зависимость между признаками и целевой переменной. Если связь между переменными нелинейная, метод может давать неточные результаты (хотя полиномиальная регрессия или другие методы могут смягчить эту проблему).
- Наличие мультиколлинеарности: В задачах с множественными признаками, если признаки сильно коррелированы, метод наименьших квадратов может приводить к неустойчивым и трудно интерпретируемым оценкам параметров.

#### Области применения:
- Задачи регрессии, где зависимость между признаками и целевой переменной линейна

### Логистическая регрессия 

**Логистическая регрессия** — это статистический метод, используемый для решения задач классификации, когда целевая переменная является **категориальной** (например, бинарной: 0 или 1). В отличие от линейной регрессии, которая предсказывает непрерывную целевую переменную, логистическая регрессия предсказывает вероятность принадлежности объекта к одному из классов.

Цель логистической регрессии — оценить вероятность того, что объект принадлежит к определённому классу, на основе одного или нескольких признаков. Чтобы преобразовать непрерывное значение, предсказываемое линейной моделью, в вероятность, логистическая регрессия использует логистическую функцию (сигмоиду)

#### Сигмоида
`σ(z) = 1 / (1 + e^(−z))​`
- `z = β0 + β1*x1 + β2*x2 + ⋯ + βp*xp` — это линейная комбинация признаков, аналогичная линейной регрессии
- `σ(z)`— вероятность того, что целевая переменная примет значение 1

#### Решение задачи классификации
- Если P(y=1∣x) ≥ 0.5, объект относится к классу 1
- Если P(y=1∣x) < 0.5, объект относится к классу 0

#### Виды логистической регрессии
1. **Бинарная логистическая регрессия**
    - Классическая форма логистической регрессии, когда целевая переменная принимает два значения (0 или 1)
2. **Мультиклассовая логистическая регрессия**
    - Используется, когда целевая переменная имеет более двух классов. Вместо одной логистической функции строится несколько моделей для предсказания вероятностей каждого класса
3. **Порядковая логистическая регрессия**
    - Применяется, когда целевая переменная — это категориальная переменная с порядком (например, "низкий", "средний", "высокий")

#### Преимущества:
- Простота и интерпретируемость: Логистическая регрессия проста в реализации и интерпретации. Коэффициенты модели показывают, как изменение каждого признака влияет на вероятность отнесения к определённому классу.
- Меньше требований к данным: В отличие от других алгоритмов, логистическая регрессия требует меньше вычислительных ресурсов и может работать с небольшими наборами данных.
- Линейность по логит-функции: Хотя логистическая регрессия предсказывает вероятность, она всё ещё остаётся линейной по своим коэффициентам, что делает её удобной для интерпретации.
- Хорошо работает при наличии большого числа признаков: Логистическая регрессия может эффективно работать в задачах с большим количеством признаков, особенно если признаки независимы друг от друга.

#### Недостатки:
- Линейность по признакам: Логистическая регрессия предполагает линейную зависимость между признаками и логарифмом шансов. Если связь между признаками и целевой переменной нелинейна, модель может не справляться с задачей.
- Неустойчивость к выбросам: Логистическая регрессия, как и линейная регрессия, чувствительна к выбросам в данных.
- Не подходит для сложных зависимостей: Для задач с более сложными и нелинейными зависимостями между признаками и целевой переменной логистическая регрессия может показывать худшие результаты по сравнению с более сложными моделями, такими как деревья решений или нейронные сети.

#### Области применения:
- Задач классификации, когда целевая переменная является категориальной
- Примеры:
	- Кредитный скоринг
	- Замеры успешности проводимых рекламных кампаний
	- Прогноз прибыли с определённого товара
	- Оценка вероятности землетрясения в конкретную дату

### Метод опорных векторов  

**Метод опорных векторов** — это мощный алгоритм машинного обучения, который используется для задач классификации и регрессии. Основная идея метода заключается в том, чтобы найти такую **гиперплоскость**, которая максимально разделяет два класса в пространстве признаков, оставляя максимально возможный зазор (или "коридор") между ближайшими точками обоих классов, которые называются **опорными векторами**. Исходя из того что объект, находящийся в N-мерном пространстве, относится к одному из двух классов, метод опорных векторов строит гиперплоскость с мерностью (N – 1), чтобы все объекты оказались в одной из двух групп.

#### Преимущества:
- Эффективность в высоких размерностях: SVM хорошо работает с данными, которые имеют много признаков (высокая размерность), благодаря тому, что опирается только на опорные векторы.
- Использование ядер: SVM может справляться с нелинейными зависимостями в данных благодаря использованию ядровых функций.
- Гибкость: SVM позволяет управлять балансом между обобщающей способностью и точностью через параметр регуляризации C.
- Хорошая обобщающая способность: Благодаря максимизации зазора между классами, SVM имеет хорошую способность к обобщению и часто показывает высокую точность на тестовых данных.

#### Недостатки:
- Чувствительность к выбору ядра: Производительность SVM сильно зависит от выбора ядровой функции и её параметров. Неправильный выбор может привести к плохой производительности.
- Высокая вычислительная сложность: В задачах с большими наборами данных SVM может быть вычислительно затратным, особенно если используются сложные ядровые функции.
- Плохо масштабируется для больших данных: Для очень больших наборов данных (с миллионами точек) SVM может быть неэффективным, так как обучение требует много памяти и времени.
- Трудность интерпретации: В отличие от, например, логистической регрессии, результаты SVM сложно интерпретировать, особенно при использовании нелинейных ядер.

#### Обоасти применения
- Задачи классификации
- Задачи регрессии
- Например
	- Распознавание образов: Классификация изображений, рукописного текста, объектов на изображениях.
	- Обработка текста: Классификация текстов, таких как анализ тональности (определение эмоциональной окраски текста) и фильтрация спама.
	- Биоинформатика: Классификация биологических данных, таких как анализ ДНК и белков.
	- Финансовый анализ: Предсказание рисков или вероятности дефолта.

### Метод ансамблей

**Ансамблевые методы** в машинном обучении — это подходы, которые объединяют предсказания нескольких моделей для получения более точных и стабильных результатов по сравнению с использованием одной модели. Идея ансамблей заключается в том, что несколько "слабых" моделей (которые по отдельности могут давать не самые точные предсказания) могут в совокупности сформировать "сильную" модель, которая обобщает лучше и показывает более высокие результаты.

#### Почему работают ансамблевые методы?
Ансамбли эффективны, потому что ошибки отдельных моделей могут компенсировать друг друга. Когда несколько моделей комбинируются, особенно если они имеют различия в подходах или слабостях, их коллективное предсказание может быть более точным и устойчивым к переобучению.

#### Основные типы ансамблевых методов:

1. **Бэггинг (Bagging)**:
    - Сокращение от **Bootstrap Aggregating**.
    - Модели обучаются на разных подвыборках данных, которые создаются с помощью метода бутстрэп (случайная выборка с возвратом из исходного набора данных).
    - Затем все модели делают предсказания, и их результаты усредняются (в задачах регрессии) или происходит голосование (в задачах классификации).
    - **Random Forest** — популярный пример бэггинга. В Random Forest создается множество деревьев решений, каждое из которых обучается на случайной подвыборке данных и случайном подмножестве признаков. Затем предсказания всех деревьев усредняются или выбирается наиболее частое предсказание.

2. **Бустинг (Boosting)**:
    - В этом методе модели обучаются последовательно, и каждая новая модель пытается исправить ошибки предыдущей.
    - Начальная модель обучается на всех данных, затем следующий классификатор акцентирует внимание на тех примерах, которые были неправильно предсказаны предыдущими моделями.
    - Модели объединяются взвешенным образом, где каждая последующая модель имеет больший вес в тех областях, где были ошибки.
    - Примеры бустинга:
        - **AdaBoost** (Adaptive Boosting) — акцентирует внимание на ошибочно классифицированных примерах, усиливая их влияние на обучение.
        - **Gradient Boosting** — использует метод градиентного спуска для последовательного обучения моделей, где каждая следующая модель пытается минимизировать ошибку предсказаний предыдущих.
        - **XGBoost** и **LightGBM** — оптимизированные версии градиентного бустинга, которые показывают высокую производительность на больших данных.

3. **Стэкинг (Stacking)**:
    - Этот метод сочетает несколько моделей разного типа (например, дерево решений, логистическая регрессия, нейронная сеть и т.д.) и обучает метамодель (модель второго уровня), которая использует предсказания всех базовых моделей в качестве входных данных.
    - Метамодель учится на этих предсказаниях и делает финальное предсказание.
    - Стэкинг позволяет учитывать разнообразие моделей и использовать их сильные стороны в разных частях данных.

4. **Ансамбли на основе голосования (Voting Ensembles)**:
    - Используют несколько разных моделей, и финальное предсказание делается с помощью голосования:
        - **Мажоритарное голосование (Majority Voting)**: в задаче классификации предсказание делается на основе выбора класса, за который проголосовало большинство моделей.
        - **Усреднение предсказаний (Averaging)**: в задачах регрессии предсказания всех моделей усредняются для получения итогового результата.
  
### Алгоритмы кластеризации  

**Алгоритмы кластеризации** — это методы машинного обучения, используемые для группировки (кластеризации) данных на основе их схожести или близости. В кластеризации данные разделяются на кластеры (группы), так что объекты внутри одного кластера имеют высокую степень схожести, а объекты из разных кластеров — низкую. Кластеризация является примером обучения без учителя, так как ей не требуется разметка данных (нет заранее известных меток классов).

Существует множество методов кластеризации, каждый из которых подходит для определённых типов данных и задач. Каждый алгоритм имеет свои сильные и слабые стороны, и выбор правильного метода зависит от структуры данных и требований задачи. Например:
- **K-means** подходит для простых задач с линейно разделимыми кластерами
- **DBSCAN** будет лучшим выбором, если данные имеют произвольные формы кластеров и содержат шум
- **Иерархическая кластеризация** хороша, когда нужно понять структуру данных на разных уровнях

#### Области применения
- Биология (исследование взаимодействия генов в геноме)
- Социология (обработка результатов социологических исследований методом Уорда)
- Информационные технологии  
  
### Метод главных компонент  

**Метод главных компонент** (Principal Component Analysis, PCA) — это метод статистического анализа и машинного обучения, который используется для уменьшения размерности данных при сохранении как можно большего объёма информации. Основная идея PCA заключается в том, чтобы преобразовать исходные данные в новое пространство меньшей размерности, при этом находя такие направления (компоненты), вдоль которых данные имеют наибольшую дисперсию.

PCA часто применяется для визуализации многомерных данных, для устранения корреляции между признаками и для уменьшения размерности данных перед использованием других алгоритмов машинного обучения. Однако метод главных компонент не годится для ситуаций, когда исходные данные слабо упорядочены (то есть все компоненты метода характеризуются высокой дисперсией). Так что его применимость определяется тем, насколько хорошо изучена и описана предметная область.  
  
### Сингулярное разложение  
  
**Сингулярное разложение матрицы** (Singular Value Decomposition, SVD) — это мощный метод линейной алгебры, который используется для факторизации матриц. Оно применяется в различных задачах, таких как уменьшение размерности данных, решение переопределённых систем линейных уравнений, сжатие данных (например, изображений) и рекомендационные системы.

SVD разлагает исходную матрицу на три другие матрицы с особыми свойствами, что позволяет легче работать с данными, выявлять важные закономерности и структуры, а также устранять шум.

#### Основная идея SVD
Для матрицы A размером m×n разложение SVD выглядит так:
`A = U Σ V^T
- `A` — исходная матрица размером m×n
- `U` — ортогональная матрица размером m×m, состоящая из **левых сингулярных векторов**
- `Σ` — диагональная матрица размером m×n, содержащая **сингулярные значения** на диагонали
- `V^T` — транспонированная ортогональная матрица размером n×nn \times nn×n, состоящая из **правых сингулярных векторов**

### Анализ независимых компонент

**Анализ независимых компонент** (Independent Component Analysis, ICA) — это метод линейной алгебры, используемый для разложения многомерных данных на статистически независимые компоненты. Это один из статистических методов, который выявляет скрытые факторы, оказывающие влияние на случайные величины, сигналы и пр. ICA формирует порождающую модель для баз многофакторных данных. Переменные в модели содержат некоторые скрытые переменные, причем нет никакой информации о правилах их смешивания. Эти скрытые переменные являются независимыми компонентами выборки и считаются негауссовскими сигналами.  

В отличие от анализа главных компонент, который связан с данным методом, анализ независимых компонент более эффективен, особенно в тех случаях, когда классические подходы оказываются бессильны. Он обнаруживает скрытые причины явлений и благодаря этому нашёл широкое применение в самых различных областях – от астрономии и медицины до распознавания речи, автоматического тестирования и анализа динамики финансовых показателей.

## Практика

### Хакатон

#### Тайминги
- до 15 октября — Регистрация участников  
- 17 октября — Установочная онлайн-встреча с организаторами  
- 22-28 октября — Работа в командах над задачами (онлайн-этап)  
- до 31 октября — Объявление финалистов (рассылка писем-приглашений на очную церемонию награждения)
- 2 ноября — Церемония награждения финалистов и победителей в Новосибирске

#### Задачи на хакатоне
1. ML
2. алгоритмическая
